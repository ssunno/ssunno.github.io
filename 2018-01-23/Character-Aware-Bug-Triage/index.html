<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Character-Aware Bug Triage · ssunno's blog</title><meta name="description" content="Character-Aware Bug Triage - Sun-Ro Lee"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://ssunno.github.io/atom.xml" title="ssunno's blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">MAIN</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/1993-11-28/info" target="_self" class="nav-list-link">INFO</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Character-Aware Bug Triage</h1><div class="post-info">Jan 23, 2018</div><div class="post-content"><h1 id="문자-기반의-Bug-Triage-최신-코드-리뷰"><a href="#문자-기반의-Bug-Triage-최신-코드-리뷰" class="headerlink" title="문자 기반의 Bug Triage 최신 코드 리뷰."></a>문자 기반의 Bug Triage 최신 코드 리뷰.</h1><ol>
<li><h4 id="이전-버전의-Bug-Triager"><a href="#이전-버전의-Bug-Triager" class="headerlink" title="이전 버전의 Bug Triager"></a>이전 버전의 Bug Triager</h4><ul>
<li><p>기본 모델: Convolutional Neural Network for Sentence Classification(Yoon Kim, 2014)</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png" alt="cnnforsentence"></p>
</li>
<li><p>전처리로 워드 임베딩(Word2Vec)을 사용함.</p>
</li>
<li>width= [3, 4, 5]인 필터를 각각 512개씩  사용 -&gt; 총 1,536개 필터</li>
<li><p>Summary, Description 필드를 각각 사용함</p>
</li>
<li><h6 id="문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함"><a href="#문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함" class="headerlink" title="문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함"></a>문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함</h6><a id="more"></a>
</li>
</ul>
<hr>
</li>
<li><h4 id="최신-구현-모델"><a href="#최신-구현-모델" class="headerlink" title="최신 구현 모델"></a>최신 구현 모델</h4><ul>
<li>기본 모델: Character-Aware Neural Language Models(Yoon Kim, 2016)<br><img src="https://pbs.twimg.com/media/CaOodS7WAAA-TLH.png" alt="canlm"></li>
<li>구조 :<ol>
<li>Character-level embedding layer</li>
<li>Term-Delayed Neural Network(TDNN)</li>
<li>Highway Network</li>
<li>Long-Short Term Memory(LSTM)</li>
<li>Predict next word</li>
</ol>
</li>
<li>Character-level embedding layer:<ul>
<li>문자 단위의 임베딩을 수행하며, Tensorflow 내장 API(tf.nn.embedding_lookup)을 사용함.</li>
<li>학습 전에 문서에서 사용되는 Character의 종류를 파악해야 함.</li>
<li>Word2Vec과 달리 모델이 학습되면서 함께 업데이트됨</li>
</ul>
</li>
<li><p>Term-Delayed Neural Network(TDNN):</p>
<ul>
<li>이전 버전 Bug Triager에서의 CNN 모듈과 같은 기능</li>
<li>논문에서는 width = [1, 2, 3, 4, 5, 6, 7], filter = [50, 100, 150, 200, 200, 200], 총 1,100개의 필터를 사용</li>
<li>implementation:<script src="//gist.github.com/ef29b19d715bb708ba3b403e8497227e.js"></script>
</li>
</ul>
</li>
<li><p>Highway Network:<br> <img src="https://cdn-images-1.medium.com/max/1600/1*R2Efhfe3zChnRcavAspQbA.png" alt="highway"></p>
<ul>
<li>implementation:<script src="//gist.github.com/cf20cbf3f36e6515d6f097058fe8224d.js"></script>
</li>
</ul>
</li>
<li><p>Long-Short Term Memory(LSTM):<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="lstm"></p>
</li>
</ul>
<hr>
<ul>
<li><p>변경된 점:</p>
<ul>
<li><p>LSTM의 마지막 output 뒤에 Fully-Connected Layer를 추가, Classification 모델로 변경함</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&apos;LSTM&apos;):</span><br><span class="line">    cell = tf.contrib.rnn.MultiRNNCell([self.__create_rnn_cell(FLAGS.rnn_size) for _ in range(FLAGS.rnn_layers)]</span><br><span class="line">                                 , state_is_tuple=True)</span><br><span class="line">    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line">    outputs, final_state = tf.contrib.rnn.static_rnn(cell, words, dtype=tf.float32)</span><br><span class="line">with tf.variable_scope(&apos;FC&apos;):</span><br><span class="line">    weight = tf.get_variable(&apos;weight&apos;, [FLAGS.rnn_size, FLAGS.num_classes], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    bias = tf.get_variable(&apos;bias&apos;, [FLAGS.num_classes], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    logits = tf.nn.xw_plus_b(outputs[-1], weight, bias)</span><br></pre></td></tr></table></figure>
</li>
<li><p>버그리포트를 학습하도록 전처리 모듈 추가</p>
</li>
<li>버그리포트에는 자연어 문장에 사용되지 않는 다양한 특수 문자가 포함되기 때문에, 단어 단위로 split(기존)하는 것이 아니라 N개의 문자 단위로 slice함.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sentence = &quot;a2b3c4d 5e6f7g8h&quot;</span><br><span class="line"># slice length : 3</span><br><span class="line">sliced = [&quot;a2b&quot;, &quot;3c4&quot;, &quot;d 5&quot;, &quot;e6f&quot;, &quot;7g8&quot;, &quot;h&lt;PAD&gt;&lt;PAD&gt;&quot;]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>실험. Classification using CNN:</p>
<ul>
<li>이전 연구에서 RNN(LSTM)이 버그리포트 분류에 적합하지 않은 것으로 보여지기 때문에 모델에서 LSTM을 CNN으로 대체한 모델을 구현함.</li>
<li><p>Implementation:</p>
<ol>
<li><p>Character-level embedding layer<br>input shape=[batch size, number of words, word length]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup([size of character dictionary, embedding size], batch data)</span><br></pre></td></tr></table></figure>
<p>output shape=[batch_size, number of words, word length, embedding_size]</p>
</li>
<li><p>Term-Delayed Neural Network(TDNN)<br>input shape=[batch size, number of words, word length, embedding size]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kernels = [1, 2, 3, 4, 5, 6, 7]</span><br><span class="line">features = [50, 100, 150, 200, 200, 200, 200]</span><br></pre></td></tr></table></figure>
<p>output shape = [batch size, number of words, sum(features)]</p>
</li>
<li><p>Highway Network<br>input shape=[batch size, number of words, sum(features)]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split_input = [tf.squeeze(x, [1]) for x in tf.split(input, number of words, 1)]</span><br><span class="line"># split_input shape=[batch size, sum(features)] * number of words</span><br><span class="line"># apply Highway and concatenate</span><br></pre></td></tr></table></figure>
<p>output shape=[batch size, number of words, sum(features)]</p>
</li>
<li><p>Convolutional Neural Network(CNN)<br>input shape=[batch size, number of words, sum(features)]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># expand for CNN (require 4D tensor), input shape=[batch_size, number of words, 1, sum(features)]</span><br><span class="line">net = conv2d(input, sum(features)//2, [3, 1], scope=&apos;conv_0&apos;)</span><br><span class="line">net = layers.max_pool2d(net, [3, 1], scope=&quot;max_pool_0&quot;)</span><br><span class="line">net = layers.dropout(net, self.dropout_keep_prob)</span><br></pre></td></tr></table></figure>
<p>output shape=[batch size, (number of words - 2)/2 - 1, 1, sum(features)//2]</p>
</li>
<li><p>Global Average Pooling layer<br>input shape=[batch size, (number of words - 2)/2 - 1, 1, sum(features)//2]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = conv2d(input, number of classes, [1, 1], scope=&apos;conv_1&apos;)</span><br><span class="line">net = avg_pool2d(net, [net.get_shape()[1], 1], stride=1,  scope=&apos;avg_pool_0&apos;)</span><br><span class="line">scores = tf.squeeze(net, [1, 2])</span><br></pre></td></tr></table></figure>
<p>output shape=[batch size, number of classes]</p>
</li>
<li>Predict Assignee<br>compare with labels</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>How to Use:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ssunno/character-aware-neural-language-models.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>Parameter list:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 train.py -h</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</div></article></div></main><footer><div class="paginator"><a href="/2017-12-22/Tensorflow로-학습된-모델의-가중치-numpy로-저장하기/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2018 <a href="http://ssunno.github.io">Sun-Ro Lee</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>