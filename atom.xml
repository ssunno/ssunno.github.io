<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ssunno&#39;s blog</title>
  
  <subtitle>블-로-그</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ssunno.github.io/"/>
  <updated>2018-01-31T06:17:00.498Z</updated>
  <id>http://ssunno.github.io/</id>
  
  <author>
    <name>Sun-Ro Lee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Character-Aware Bug Triage</title>
    <link href="http://ssunno.github.io/2018-01-23/Character-Aware-Bug-Triage/"/>
    <id>http://ssunno.github.io/2018-01-23/Character-Aware-Bug-Triage/</id>
    <published>2018-01-23T14:20:43.000Z</published>
    <updated>2018-01-31T06:17:00.498Z</updated>
    
    <content type="html"><![CDATA[<h1 id="문자-기반의-Bug-Triage-최신-코드-리뷰"><a href="#문자-기반의-Bug-Triage-최신-코드-리뷰" class="headerlink" title="문자 기반의 Bug Triage 최신 코드 리뷰."></a>문자 기반의 Bug Triage 최신 코드 리뷰.</h1><ol><li><h4 id="이전-버전의-Bug-Triager"><a href="#이전-버전의-Bug-Triager" class="headerlink" title="이전 버전의 Bug Triager"></a>이전 버전의 Bug Triager</h4><ul><li><p>기본 모델: Convolutional Neural Network for Sentence Classification(Yoon Kim, 2014)</p><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png" alt="cnnforsentence"></p></li><li><p>전처리로 워드 임베딩(Word2Vec)을 사용함.</p></li><li>width= [3, 4, 5]인 필터를 각각 512개씩  사용 -&gt; 총 1,536개 필터</li><li><p>Summary, Description 필드를 각각 사용함</p></li><li><h6 id="문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함"><a href="#문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함" class="headerlink" title="문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함"></a>문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함</h6><a id="more"></a></li></ul><hr></li><li><h4 id="최신-구현-모델"><a href="#최신-구현-모델" class="headerlink" title="최신 구현 모델"></a>최신 구현 모델</h4><ul><li>기본 모델: Character-Aware Neural Language Models(Yoon Kim, 2016)<br><img src="https://pbs.twimg.com/media/CaOodS7WAAA-TLH.png" alt="canlm"></li><li>구조 :<ol><li>Character-level embedding layer</li><li>Term-Delayed Neural Network(TDNN)</li><li>Highway Network</li><li>Long-Short Term Memory(LSTM)</li><li>Predict next word</li></ol></li><li>Character-level embedding layer:<ul><li>문자 단위의 임베딩을 수행하며, Tensorflow 내장 API(tf.nn.embedding_lookup)을 사용함.</li><li>학습 전에 문서에서 사용되는 Character의 종류를 파악해야 함.</li><li>Word2Vec과 달리 모델이 학습되면서 함께 업데이트됨</li></ul></li><li><p>Term-Delayed Neural Network(TDNN):</p><ul><li>이전 버전 Bug Triager에서의 CNN 모듈과 같은 기능</li><li>논문에서는 width = [1, 2, 3, 4, 5, 6, 7], filter = [50, 100, 150, 200, 200, 200], 총 1,100개의 필터를 사용</li><li>implementation:<script src="//gist.github.com/ef29b19d715bb708ba3b403e8497227e.js"></script></li></ul></li><li><p>Highway Network:<br> <img src="https://cdn-images-1.medium.com/max/1600/1*R2Efhfe3zChnRcavAspQbA.png" alt="highway"></p><ul><li>implementation:<script src="//gist.github.com/cf20cbf3f36e6515d6f097058fe8224d.js"></script></li></ul></li><li><p>Long-Short Term Memory(LSTM):<br><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="lstm"></p></li></ul><hr><ul><li><p>변경된 점:</p><ul><li><p>LSTM의 마지막 output 뒤에 Fully-Connected Layer를 추가, Classification 모델로 변경함</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&apos;LSTM&apos;):</span><br><span class="line">    cell = tf.contrib.rnn.MultiRNNCell([self.__create_rnn_cell(FLAGS.rnn_size) for _ in range(FLAGS.rnn_layers)]</span><br><span class="line">                                 , state_is_tuple=True)</span><br><span class="line">    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line">    outputs, final_state = tf.contrib.rnn.static_rnn(cell, words, dtype=tf.float32)</span><br><span class="line">with tf.variable_scope(&apos;FC&apos;):</span><br><span class="line">    weight = tf.get_variable(&apos;weight&apos;, [FLAGS.rnn_size, FLAGS.num_classes], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    bias = tf.get_variable(&apos;bias&apos;, [FLAGS.num_classes], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    logits = tf.nn.xw_plus_b(outputs[-1], weight, bias)</span><br></pre></td></tr></table></figure></li><li><p>버그리포트를 학습하도록 전처리 모듈 추가</p></li><li>버그리포트에는 자연어 문장에 사용되지 않는 다양한 특수 문자가 포함되기 때문에, 단어 단위로 split(기존)하는 것이 아니라 N개의 문자 단위로 slice함.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sentence = &quot;a2b3c4d 5e6f7g8h&quot;</span><br><span class="line"># slice length : 3</span><br><span class="line">sliced = [&quot;a2b&quot;, &quot;3c4&quot;, &quot;d 5&quot;, &quot;e6f&quot;, &quot;7g8&quot;, &quot;h&lt;PAD&gt;&lt;PAD&gt;&quot;]</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>실험. Classification using CNN:</p><ul><li>이전 연구에서 RNN(LSTM)이 버그리포트 분류에 적합하지 않은 것으로 보여지기 때문에 모델에서 LSTM을 CNN으로 대체한 모델을 구현함.</li><li><p>Implementation:</p><ol><li><p>Character-level embedding layer<br>input shape=[batch size, number of words, word length]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup([size of character dictionary, embedding size], batch data)</span><br></pre></td></tr></table></figure><p>output shape=[batch_size, number of words, word length, embedding_size]</p></li><li><p>Term-Delayed Neural Network(TDNN)<br>input shape=[batch size, number of words, word length, embedding size]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kernels = [1, 2, 3, 4, 5, 6, 7]</span><br><span class="line">features = [50, 100, 150, 200, 200, 200, 200]</span><br></pre></td></tr></table></figure><p>output shape = [batch size, number of words, sum(features)]</p></li><li><p>Highway Network<br>input shape=[batch size, number of words, sum(features)]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split_input = [tf.squeeze(x, [1]) for x in tf.split(input, number of words, 1)]</span><br><span class="line"># split_input shape=[batch size, sum(features)] * number of words</span><br><span class="line"># apply Highway and concatenate</span><br></pre></td></tr></table></figure><p>output shape=[batch size, number of words, sum(features)]</p></li><li><p>Convolutional Neural Network(CNN)<br>input shape=[batch size, number of words, sum(features)]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># expand for CNN (require 4D tensor), input shape=[batch_size, number of words, 1, sum(features)]</span><br><span class="line">net = conv2d(input, sum(features)//2, [3, 1], scope=&apos;conv_0&apos;)</span><br><span class="line">net = layers.max_pool2d(net, [3, 1], scope=&quot;max_pool_0&quot;)</span><br><span class="line">net = layers.dropout(net, self.dropout_keep_prob)</span><br></pre></td></tr></table></figure><p>output shape=[batch size, (number of words - 2)/2 - 1, 1, sum(features)//2]</p></li><li><p>Global Average Pooling layer<br>input shape=[batch size, (number of words - 2)/2 - 1, 1, sum(features)//2]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = conv2d(input, number of classes, [1, 1], scope=&apos;conv_1&apos;)</span><br><span class="line">net = avg_pool2d(net, [net.get_shape()[1], 1], stride=1,  scope=&apos;avg_pool_0&apos;)</span><br><span class="line">scores = tf.squeeze(net, [1, 2])</span><br></pre></td></tr></table></figure><p>output shape=[batch size, number of classes]</p></li><li>Predict Assignee<br>compare with labels</li></ol></li></ul></li></ul><hr><ul><li><p>How to Use:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ssunno/character-aware-neural-language-models.git</span><br></pre></td></tr></table></figure></li><li><p>Parameter list:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 train.py -h</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;문자-기반의-Bug-Triage-최신-코드-리뷰&quot;&gt;&lt;a href=&quot;#문자-기반의-Bug-Triage-최신-코드-리뷰&quot; class=&quot;headerlink&quot; title=&quot;문자 기반의 Bug Triage 최신 코드 리뷰.&quot;&gt;&lt;/a&gt;문자 기반의 Bug Triage 최신 코드 리뷰.&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;h4 id=&quot;이전-버전의-Bug-Triager&quot;&gt;&lt;a href=&quot;#이전-버전의-Bug-Triager&quot; class=&quot;headerlink&quot; title=&quot;이전 버전의 Bug Triager&quot;&gt;&lt;/a&gt;이전 버전의 Bug Triager&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;기본 모델: Convolutional Neural Network for Sentence Classification(Yoon Kim, 2014)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png&quot; alt=&quot;cnnforsentence&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;전처리로 워드 임베딩(Word2Vec)을 사용함.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;width= [3, 4, 5]인 필터를 각각 512개씩  사용 -&amp;gt; 총 1,536개 필터&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Summary, Description 필드를 각각 사용함&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;h6 id=&quot;문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함&quot;&gt;&lt;a href=&quot;#문제-대형-프로젝트-Assignee-1-000개-이상-에서-개발자를-분류해-내지-못함&quot; class=&quot;headerlink&quot; title=&quot;문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함&quot;&gt;&lt;/a&gt;문제: 대형 프로젝트(Assignee 1,000개 이상)에서 개발자를 분류해 내지 못함&lt;/h6&gt;
    
    </summary>
    
      <category term="Python" scheme="http://ssunno.github.io/categories/Python/"/>
    
    
      <category term="Others" scheme="http://ssunno.github.io/tags/Others/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow로 학습된 모델의 가중치 numpy로 저장하기</title>
    <link href="http://ssunno.github.io/2017-12-22/Tensorflow%EB%A1%9C-%ED%95%99%EC%8A%B5%EB%90%9C-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EA%B0%80%EC%A4%91%EC%B9%98-numpy%EB%A1%9C-%EC%A0%80%EC%9E%A5%ED%95%98%EA%B8%B0/"/>
    <id>http://ssunno.github.io/2017-12-22/Tensorflow로-학습된-모델의-가중치-numpy로-저장하기/</id>
    <published>2017-12-22T06:23:24.000Z</published>
    <updated>2018-01-23T14:48:50.148Z</updated>
    
    <content type="html"><![CDATA[<p>모델을 구성하는 Tensor 리스트를 가지고 있다가, 학습이 끝난 뒤에 weight을 직접 추출해서 numpy로 저장하면 된다.</p><script src="//gist.github.com/d294e1d5c2fb1183b75a61bdde420adf.js"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;모델을 구성하는 Tensor 리스트를 가지고 있다가, 학습이 끝난 뒤에 weight을 직접 추출해서 numpy로 저장하면 된다.&lt;/p&gt;
&lt;script src=&quot;//gist.github.com/d294e1d5c2fb1183b75a61bdde420a
      
    
    </summary>
    
      <category term="Tensorflow" scheme="http://ssunno.github.io/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://ssunno.github.io/tags/Tensorflow/"/>
    
      <category term="numpy" scheme="http://ssunno.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>futures 이용한 멀티쓰레딩/멀티프로세싱</title>
    <link href="http://ssunno.github.io/2017-12-22/futures-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A9%80%ED%8B%B0%EC%93%B0%EB%A0%88%EB%94%A9-%EB%A9%80%ED%8B%B0%ED%94%84%EB%A1%9C%EC%84%B8%EC%8B%B1/"/>
    <id>http://ssunno.github.io/2017-12-22/futures-이용한-멀티쓰레딩-멀티프로세싱/</id>
    <published>2017-12-22T04:04:49.000Z</published>
    <updated>2017-12-22T06:26:32.415Z</updated>
    
    <content type="html"><![CDATA[<p>파이썬 concurrent.futures 모듈을 이용한 멀티쓰레딩/멀티프로세싱</p><ul><li>ProcessPoolExecutor 대신 ThreadPoolExecutor를 쓰면 멀티쓰레드로 동작</li></ul><script src="//gist.github.com/231ce513f265b42a8db62fc11ee36fee.js"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;파이썬 concurrent.futures 모듈을 이용한 멀티쓰레딩/멀티프로세싱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ProcessPoolExecutor 대신 ThreadPoolExecutor를 쓰면 멀티쓰레드로 동작&lt;/li&gt;
&lt;/ul&gt;
&lt;script src=&quot;
      
    
    </summary>
    
      <category term="Python" scheme="http://ssunno.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://ssunno.github.io/tags/Python/"/>
    
      <category term="Multiprocessing" scheme="http://ssunno.github.io/tags/Multiprocessing/"/>
    
  </entry>
  
  <entry>
    <title>수집 자료 리스트</title>
    <link href="http://ssunno.github.io/2017-12-22/%EC%9E%90%EB%A3%8C%20%EB%A6%AC%EC%8A%A4%ED%8A%B8/"/>
    <id>http://ssunno.github.io/2017-12-22/자료 리스트/</id>
    <published>2017-12-22T04:02:08.000Z</published>
    <updated>2017-12-22T04:02:15.797Z</updated>
    
    <content type="html"><![CDATA[<h3 id="자료-목록"><a href="#자료-목록" class="headerlink" title="자료 목록"></a>자료 목록</h3><ul><li>Convolutional Neural Network for Sentence Classification [Kim, 2014]<ul><li><a href="http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf" target="_blank" rel="noopener">http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf</a></li><li>내가 최초에 참고했던, CNN 기반 문서 분류모델</li></ul></li><li>Recent Trends in Deep Learning Based Natural Language Processing [Young, 2017]<ul><li><a href="https://arxiv.org/pdf/1708.02709.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.02709.pdf</a></li><li>딥러닝 기반 자연어 기법을 정리한 Survey</li><li><a href="https://ratsgo.github.io/natural%20language%20processing/2017/08/16/deepNLP/" target="_blank" rel="noopener">한글 번역 블로그</a></li></ul></li><li>CS224n: Natural Language Processing with Deep Learning<ul><li><a href="http://web.stanford.edu/class/cs224n/index.html" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/index.html</a></li><li>스탠포드 대학의 딥러닝 기반 자연어 처리 강의</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;자료-목록&quot;&gt;&lt;a href=&quot;#자료-목록&quot; class=&quot;headerlink&quot; title=&quot;자료 목록&quot;&gt;&lt;/a&gt;자료 목록&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network for Sentence Classifica
      
    
    </summary>
    
      <category term="자연어 처리" scheme="http://ssunno.github.io/categories/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/"/>
    
    
      <category term="Natural Language Processing" scheme="http://ssunno.github.io/tags/Natural-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>info</title>
    <link href="http://ssunno.github.io/1993-11-28/info/"/>
    <id>http://ssunno.github.io/1993-11-28/info/</id>
    <published>1993-11-28T03:34:56.000Z</published>
    <updated>2018-01-31T06:54:49.903Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Publication"><a href="#Publication" class="headerlink" title="Publication"></a>Publication</h4><ol><li>Applying Deep Learning based Bug Triager to Industrial Projects [<a href="http://dl.acm.org/citation.cfm?id=3117776&amp;CFID=793472496&amp;CFTOKEN=71905086&amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BwwAJTAGASly9KBfdDM4voA%3D%3D" target="_blank" rel="noopener">link</a>]<ul><li>학술대회명: 11th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT symposium on the Foundations of Software Engineering(ESEC/FSE)</li><li>게재연월: 2017. 09</li></ul></li><li>딥러닝을 이용한 버그 담당자 자동 배정 연구 [<a href="http://sigsoft.or.kr/ftp/KCSE2017_Proceedings.pdf?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BwwAJTAGASly9KBfdDM4voA%3D%3D" target="_blank" rel="noopener">link</a>]<ul><li>학술대회명: 한국 소프트웨어공학 학술대회(KCSE)</li><li>게재연월: 2017.02</li></ul></li><li>딥러닝을 이용한 버그 담당자 자동 배정 연구 [<a href="http://www.kiise.or.kr/academy/board/publishList2.fa?MENU_ID=060500" target="_blank" rel="noopener">link</a>]<ul><li>학술지명: 정보과학회논문지</li><li>게재연월 및 역할: 2017.11 / 주저(제1저자)</li></ul></li><li>Deep Learning based real-time query processing for wireless network [<a href="http://journals.sagepub.com/doi/abs/10.1177/1550147717707896?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BwwAJTAGASly9KBfdDM4voA%3D%3D" target="_blank" rel="noopener">link</a>]<ul><li>학술지명: International Journal of Distributed Sensor Networks(IJDSN)</li><li>게재연월: 2017.04</li></ul></li><li>T-L Plane Abstraction-Based Energy-Efficient Real-Time Scheduling for Multi-Core Wireless Sensors [<a href="http://www.mdpi.com/1424-8220/16/7/1054/htm?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BwwAJTAGASly9KBfdDM4voA%3D%3D" target="_blank" rel="noopener">link</a>]<ul><li>학술지명: Sensors</li><li>게재연월: 2016.07</li></ul></li></ol><h4 id="links"><a href="#links" class="headerlink" title="links"></a>links</h4><ul><li>facebook: <a href="https://www.facebook.com/ssunnohd" target="_blank" rel="noopener">https://www.facebook.com/ssunnohd</a></li><li>Github: <a href="https://github.com/ssunno" target="_blank" rel="noopener">https://github.com/ssunno</a></li><li>instagram: <a href="https://www.instagram.com/ssunno" target="_blank" rel="noopener">https://www.instagram.com/ssunno</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Publication&quot;&gt;&lt;a href=&quot;#Publication&quot; class=&quot;headerlink&quot; title=&quot;Publication&quot;&gt;&lt;/a&gt;Publication&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Applying Deep Learning based
      
    
    </summary>
    
    
  </entry>
  
</feed>
